---
title: Summary of Initial Survey
---

# Summary of Survey "P8: Correction of open-ended exam questions through automatization"

#survey #summary

This document summarizes the results of the online-survey on the correction of open-ended exam questions through automatization.

### Who participated?

45 people completed our survey and provided valuable feedback. The below graphs summarize the roles of the participants, how they are involved in the grading process, and in which academic area they conduct their exams.

![png](research/survey/plots/output_2_0.png)

### How do the participants conduct their exams?

Exams at our university differ from each other in several dimensions. Exams can differ regarding their type (e.g. paper-based in lecture hall), what materials the students are allowed to use, and what type of questions the exams contain (e.g. multiple choice). The below graphs show what types of exams, which material use, and which types of questions are most common in the courses of our participants.

![png](research/survey/plots/output_4_0.png)

### What motivates examiners to include open-ended questions?

There are several reasons an examiner may include open-ended questions in an exam. One question in our survey focused on what the most beneficial aspects of open-ended questions are.

_In your opinion, what are the most beneficial factors of open-ended questions? Please rank the factors by assigning their relative rank compared to the other options._

![png](research/survey/plots/output_6_0.png)

According to the participants of our survey, the most beneficial aspect of open-ended questions is the possibility to better assess the students' knowledge (average ranking 1.76), followed by the improved quality of the students' answers (e.g. harder to guess, more detailed answers etc) (2.34). The other included aspects are, on average, perceived to be considerably less beneficial. The mean and standard deviation of the ranking of each beneficial aspects can be found in the appendix.

### What challenges arise from open-ended questions?

While open-ended questions have beneficial aspects, they also present challenges to examiners. One question focused on how challenging different aspects of grading open-ended questions are.

_In your opinion, how challenging are these factors regarding the correction of open-ended exam questions? Please classify each of the following factors according to their level of challenge._

![png](research/survey/plots/output_9_0.png)

The participants perceived the workload related to the grading of open-ended questions as the biggest challenge (average rating 3.95/5), followed by the inconsistency of examiners (e.g. allocation of a different number of points for the "same" answer) (3.50) and the formulation of transparent grading criteria (3.20). Other aspects that were perceived as relatively challenging were the systematic assignment of grades (3.13), handwriting that is hard to read (3.07), varying personal constitution (3.05), the quality of the students' responses (3.05), the inconsistency of multiple examiners grading the same exam (2.96), and the misunderstanding of unclear questions (2.93). The other aspects of open-ended question we included in our survey were, on average, perceived to be less challenging. The mean and standard deviation of the rating of each challenge can be found in the appendix.

### How widespread is the use of software for grading open-ended questions among the participants?

One question in our survey focused on whether our participants already used software for the grading of open-ended questions.

_Have you previously used software with the specific goal of improving the grading process of open-ended questions? If yes, what software have you used? If no, do you think software could help you with the grading process? How?_

Of the 27 participants who have explicitly stated whether they have used software for grading open-ended questions, 10 answered yes (i.e. 37%). Software listed by the participants includes Ans, SEB, EPIS, a tool developed by the Teaching Center to grade Excel exams, and, in one case, even a tool that was developed by a participant himself. Of the people who have never used software, five explicitly stated that they think software would be useful (e.g. "Software might help in making a scheme of points to be reviewed when choosing the grade for an open question") and ten explicitly stated that they do not think there exists useful software that facilitates the grading of open-ended questions (e.g. “I would be surprised if there were a software that did a reasonably good job at this”).

### What features do the participant think would be useful?

The use of software could, at least partially, address some of the challenges arising from open-ended questions. One question in our survey focused on which features of a software would be helpful for the grading process.

_Please classify each of the following concepts according to their importance when focusing on the correction of open-ended exam questions._

![png](research/survey/plots/output_13_0.png)

The feature conisdered as the by far the most important by the participants is the possibility to specify consistent correction criteria, a concept known as rubrics (average rating 4.46). Other features considered as relevant are the possibility to automatically group similar answers (3.60), anonymous correction (3.48), text similarity analysis (3.46), cheating detection (3.40), and the possibility to write manual feedback (3.31). The other features we included in our survey were, on average, perceived to be less important. The mean and standard deviation of the rating of each feature can be found in the appendix.

### Additional Remarks of Participants

Our participants shared a lot of very helpful remarks with us. They ranged from suggestions for additional resources (e.g. "I think it would be interesting to have a general document describing how are failing grades/grading curves used") to affirmation of the importance of open-ended questions (e.g. "Having been a student here myself, I think multiple choice questions provide bad incentives for learning; it would be much better if they were replaced with open-ended questions, for which automated tools could be very valuable"). Moreover, skepticism towards automated solution was widespread in the remarks (e.g. "I think that open ended exam questions shouldn't be automated", "Apart from multiple choice, having computers try to grade exams is a waste of time and resources").

### Appendix

1. Table with mean and standard deviation of the ranking of different motivations for including open-ended questions in exams:

|                                   | Description in Survey                                                                                                                  | Mean | StdDev |
| --------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------- | ---- | ------ |
| Better Assessment of Knowledge    | Better assessment of knowledge (e.g., higher cognitive thinking levels, assessing recall, broader knowledge)                           | 1.76 | 1.12   |
| Quality of Responses              | Improved quality of responses (e.g., harder to guess, more detailed responses, broader range of possible answers)                      | 2.35 | 1.12   |
| Preparation by Students           | More thorough preparation by students (e.g., because students study more for open-ended questions)                                     | 3.33 | 1.23   |
| Bias in Responses                 | Less bias in responses (e.g., because response options are not specified)                                                              | 3.78 | 1.03   |
| Flexibility in Design and Grading | Flexibility in grading and design (e.g., no formatting requirements, accounting for consequential errors, fits lecture content better) | 4.15 | 1.35   |
| Other                             | Other (please specify)                                                                                                                 | 5.63 | 1.16   |

2. Table with mean and standard deviation of ratings of all included challenges arising from open-ended questions:

|                                        | Description in Survey                                                                                                           | Mean | StdDev |
| -------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------- | ---- | ------ |
| Workload                               | The overall workload of the correction process                                                                                  | 3.95 | 1.04   |
| Inconsistency of Same Examiner         | Individual correction differences (e.g., allocating a different number of points for same answers)                              | 3.50 | 1.09   |
| Formulation of transparent Criteria    | Formulating transparent solution criteria to provide feedback to the students (e.g., during exam review)                        | 3.20 | 1.18   |
| Systematic Assignment of Grades        | Assigning grades systematically (e.g., defining a fair grading scheme)                                                          | 3.13 | 1.18   |
| Hard to read Handwriting               | Hard to read handwriting                                                                                                        | 3.07 | 1.27   |
| Varying Personal Constitution          | Varying personal constitution throughout the exam correction (e.g., attention span, motivation)                                 | 3.05 | 1.13   |
| Quality of Student Responses           | Quality of student responses (e.g., students guessing or listing a lot of unrelated information hoping to get points by chance) | 3.05 | 1.24   |
| Inconsistency of Multiple Examiner     | Differing expectations between examiners (when there is more than one person correcting the exam)                               | 2.98 | 1.10   |
| Misunderstandings of Unclear Questions | Handling misunderstandings when questions are not formulated clearly (e.g., awarding points for unclear questions)              | 2.93 | 1.14   |
| Detect Cheating                        | Cheating detection (e.g., detecting similar answers)                                                                            | 2.56 | 1.23   |
| Calculation of Statistics for Analysis | Calculation of assignment and question statistics for later analysis (e.g., hard questions, item analysis)                      | 2.50 | 1.03   |
| Detect Plagiarism                      | Detecting plagiarism (e.g., missing references)                                                                                 | 2.34 | 1.09   |
| Bias of Examiner                       | Impartiality and inadvertent biases of the examiner (e.g., discrimination because of name)                                      | 2.03 | 1.03   |

3. Table with mean and standard deviation of ratings of all included features:

|                                                                    | Description in Survey                                                                                                                                    | Mean | StdDev |
| ------------------------------------------------------------------ | -------------------------------------------------------------------------------------------------------------------------------------------------------- | ---- | ------ |
| [[research/features/definitions/Rubrics]]                          | Consistent correction criteria across students (e.g., by establishing correction criteria and grading by selecting applicable criteria for each student) | 4.46 | 0.79   |
| [[research/features/definitions/Grouping]]                         | Grouping of similar responses to improve efficiency (e.g., to grade all responses with the same content consistently and in fewer clicks)                | 3.60 | 1.20   |
| [Anonymous Correction](research/features/definitions/Anonymity.md) | Anonymous correction so that graders cannot identify students                                                                                            | 3.49 | 1.14   |
| [[research/features/definitions/Text Similarity Analysis]]         | Text similarity analysis between students (e.g., for essay questions)                                                                                    | 3.46 | 1.19   |
| [[research/features/definitions/Cheating Detection]]               | Cheating detection (e.g., based on statistical approaches or response content)                                                                           | 3.40 | 1.35   |
| [[research/features/definitions/Manual Feedback]]                  | Giving manual feedback to students (e.g., making comments on mistakes when grading)                                                                      | 3.32 | 1.28   |
| [[research/features/definitions/Collaboration]]                    | Possibility to distribute grading workload over different people (collaborative / delegated grading)                                                     | 3.23 | 1.25   |
| [[research/features/definitions/Text Recognition]]                 | Conversion of handwriting to text for easier processing                                                                                                  | 3.05 | 1.23   |
| [[research/features/definitions/Statistics]]                       | Assignment and question statistics (e.g., automated computation of question difficulty, item analysis)                                                   | 3.02 | 1.26   |
| [[research/features/definitions/Fully Automated Grading]]          | Fully automated points calculation (e.g., based on rules, solution space, or other automated approaches)                                                 | 2.90 | 1.37   |
| [[research/features/definitions/Automated Grading Suggestions]]    | Automated suggestion of correction criteria and points for each student (i.e., with manual approval)                                                     | 2.87 | 1.27   |
| [[research/features/definitions/Exam Review]]                      | Possibility for online-exam review for the students (e.g., with a login to the platform)                                                                 | 2.87 | 1.20   |
| [[research/features/definitions/Code Similarity Analysis]]         | Code similarity analysis for programming questions                                                                                                       | 2.86 | 1.29   |
