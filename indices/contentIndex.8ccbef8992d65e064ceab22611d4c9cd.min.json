{"/":{"title":"P-8 Automated Correction","content":"---\n\nWelcome to the knowledge base of the project on _\"Automated Correction of Open-ended Exam Questions\"_ that we currently work on at the [BF Teaching Center](https://www.bf.uzh.ch/de/studies/general/teaching-center.html). Our knowledge base contains a summarization of the work we are doing, as well as the resources we provide for your own application.\n\n## Mission\n\nA key requirement when performing any kind of exam correction is to guarantee a **valid, objective** and **fair** **grading process**. There is often some room for interpretation when grading, especially in **open-ended** exam questions, where students generate answers themselves (i.e., the solution space is much broader than with, e.g., single-choice questions). **Procedural fairness** – referring to the perception of correctability and consistency – is of importance in the design, conduction and correction process of any exam, and is at the core of our project.\n\nOur primary goal is to evaluate whether and how the correction of exams could be supported with **automated tools and procedures**. (Semi-)automated correction of open-ended exam questions has the potential to reduce human subjective judgement, improve fairness for the students, as well as allow lecturers to grade in a more structured and efficient way.\n\n## Milestones\n\nThe primary milestones in our project are the following:\n\n| Timeframe  | Description                                 | Resources                                                  |\n|","lastmodified":"2022-04-11T14:32:20.880624273Z","tags":null},"/notes/research/challenges/Consistency":{"title":"Untitled Page","content":"# Consistency\n#challenge \n\nGrading open-ended questions consistently is a big challenge according to the participants of our [survey](research/survey/Summary.md). The goal is that same (similar) answers are awarded the same (a similar) amount of points.\n\nThere are several possible sources of inconsistency:\n- Same grader is inconsistent across students\n  - [Biases](research/challenges/biases/Biases.md) systematically affect the grading\n  - There is an (unsystematic) error when awarding points due to the fact that humans' judgment is inconsistent\n\n- Multiple grades have different views on same exercise\n\nMany of the sources of inconsistency can be adressed through the use of software, e.g. through [[research/features/definitions/Rubrics]].","lastmodified":"2022-04-11T14:32:20.880624273Z","tags":null},"/notes/research/challenges/Workload":{"title":"Untitled Page","content":"# Workload\n#challenge\n\nThe workload connected with grading open-ended questions is a big challenge, especially in large courses. This incentivizes people to use other question types (such as multiple choice) in their exams. According to the participants of our [survey](research/survey/Summary.md), the workload is the biggest obstacle to the incorporation of open-ended questions in exams.\n\nThis challenge must be adressed if people should be encouraged to use more open-ended questions in their exams. This can partly be achieved through the [[research/features/definitions/Digitization]] of the grading process and, related to this, a well-designed [[research/features/definitions/Grading Workflow]]. In order to reduce the workload of the grading process even further, parts of the grading must be automated (see [[research/concepts/Semiautomated Grading]] and [[research/concepts/Automated Grading]]).","lastmodified":"2022-04-11T14:32:20.880624273Z","tags":null},"/notes/research/challenges/biases/Anchoring":{"title":"Untitled Page","content":"# Anchoring \n#bias\n\nThe Anchoring Effect is a well-researched cognitive bias (see [Wikipedia Page](https://en.wikipedia.org/wiki/Anchoring_(cognitive_bias))) whereby people's decision are influenced by a particular reference points (anchor). \n\nWhile the other listed biases can be adresses through the use of software, this bias would potentially arise from the use of software if software is used for the [[research/features/definitions/Sorting]] of ansers. If some answer is sorted into a cohort which has some positive (negative) attributes (e.g. a keyword the examiner is looking for) this setting may present an anchor and bias the examiner to award more (less) points. ","lastmodified":"2022-04-11T14:32:20.880624273Z","tags":null},"/notes/research/challenges/biases/Biases":{"title":"Untitled Page","content":"# Overview Biases\nWe hereby list biases or causes of them which could potentially affect the grading process. It is important to note that we do not provide evidence that these biases actually occur in the grading process.\n\nPotential Biases:\n- [[research/challenges/biases/Discrimination]]\n- [[research/challenges/biases/Context]]\n- [[research/challenges/biases/Halo]]\n- [[research/challenges/biases/Mental Depletion]] \n- [[research/challenges/biases/Anchoring]]\n- [[research/challenges/biases/Presentation of Answer]]\n","lastmodified":"2022-04-11T14:32:20.880624273Z","tags":null},"/notes/research/challenges/biases/Context":{"title":"Untitled Page","content":"# Context Effects\n#bias \n\nContext effects (see [Wikipedia Page](https://en.wikipedia.org/wiki/Context_effect)) describe the influence of environmental factors on ones perception of a stimulus.\n\nContext effects could bias the grading in the following way...","lastmodified":"2022-04-11T14:32:20.880624273Z","tags":null},"/notes/research/challenges/biases/Discrimination":{"title":"Untitled Page","content":"# Discrimination\n#bias \n\nThe name of the student may have an effect on the numbers of points awarded (e.g. through gender or race bias). There exists a substantial amount of literature on this topic (see [Google Scholar](https://scholar.google.ch/scholar?hl=de\u0026as_sdt=0%2C5\u0026q=grading+discrimination\u0026oq=grading+discrim)).\n\nThis bias may occur if the exams are not anonymized. [[research/features/definitions/Anonymity]] would mitigate large parts of this problem. It, however, may still be possible for the grader to identify the sex of the student based on the handwriting. This could be resolved if [[research/features/definitions/Text Recognition]] is used and the grader is presented machine written text.\n\n","lastmodified":"2022-04-11T14:32:20.880624273Z","tags":null},"/notes/research/challenges/biases/Halo":{"title":"Untitled Page","content":"# Halo\n#bias \n\nThe \"Halo Effect\" is well-researched cognitive bias (see [Wikipedia Page](https://en.wikipedia.org/wiki/Halo_effect)) which can affect people's judgment in different situations. On a high level, the halo effect describes the tendency of evaluators to be influenced by previous judgments of personality or performance of an entity. \n\nThis bias may influence an examiner during the grading process. If the first exercise is solved well (badly) by a student, the grader will tend to award more (less) points than accurate for the other exercises of the student. This constellation may arise if the grader processes the exams sequentially student by student. The grader could also be influenced if he remembers older assessments of the student or contributions of the student in the course. This may arise if the answers are not anonymized.\n\n[[research/features/definitions/Anonymity]] and a well-designed [[research/features/definitions/Grading Workflow]] would eliminate this bias.\n","lastmodified":"2022-04-11T14:32:20.880624273Z","tags":null},"/notes/research/challenges/biases/Mental-Depletion":{"title":"Untitled Page","content":"# Mental Depletion\n#bias \n\nMental depletion may bias the judgment of people. A situation where this bias manifests is a the [\"Hungry Judge Effect\"](https://en.wikipedia.org/wiki/Hungry_judge_effect). \n\nA similar situation could occur when the examiner tends to award less points in the end of a grading session. If the same student is always graded at the end, this may have a substantial effect on the points of students.\n\n[[research/features/definitions/Shuffling]] could adress parts of these bias as the students that are graded in the end are not the same for every exercise.","lastmodified":"2022-04-11T14:32:20.880624273Z","tags":null},"/notes/research/challenges/biases/Presentation-of-Answer":{"title":"Untitled Page","content":"# Presentation of Answer\n#bias \n\nThe way student presents an answer may have an effect on the points he is awarded, also in settings were the focus should solely lay on the content. \n\nA student with a bad handwriting or poor language skills may be awarded less points than appropriate. \n\n[[research/features/definitions/Text Recognition]] would adress this bias for text answers if the grader is presented machine-written text.","lastmodified":"2022-04-11T14:32:20.880624273Z","tags":null},"/notes/research/concepts/Assisted-Grading":{"title":"Untitled Page","content":"# Assisted Grading\n#concept #definition \n\nAssisted Grading facilitates the grading process by assisting the grader in their grading process, e.g. by improving the efficiency of the workflow of the grader. \n\nIn contrast to [[research/concepts/Automated Grading]] and [[research/concepts/Semiautomated Grading]], Assissted Grading software includes only features which do not have a direct impact (e.g. through suggestions) on the points that the students are awarded (see [[research/features/Feature Categories]]).","lastmodified":"2022-04-11T14:32:20.880624273Z","tags":null},"/notes/research/concepts/Automated-Grading":{"title":"Untitled Page","content":"# Automated Grading\n#concept #definition \n\nAutomated grading facilitates the grading process by automating all aspects of grading with software. Automated grading is conducted without human verification of the grading. Humans may still control samples in order to ensure the validity of the automated grading.","lastmodified":"2022-04-11T14:32:20.880624273Z","tags":null},"/notes/research/concepts/Handwriting-Recognition":{"title":"Untitled Page","content":"# Handwriting Recognition\n#concept\n\nRecognize text from handwritten documents. This is a way harder problem than recognizing printed text as the variability of handwritten text can be way higher.\n\nHandwriting text recognition can either be conducted:\n- **\"Offline\"** (no information on pen movement available)\n  Offline text recognition uses [[research/concepts/Optical Character Recognition]] techniques.  Tools where \"offline\" handwriting recognition is supported are, for example, [[Azure OCR API]] and [[Google Keep Notes]]. \n\n- **\"Online\"** (information on pen movement available)\n  Since in \"online\" text recognition more information is available to the computer it outperforms \"offline\" text recognition. An tool where \"online\" handwriting recognition is supported is [[Microsoft One Note]]. Online text recognition works considerably better than offline text recognition.\n\nHandwriting recognition works way better for english than for german text (see e.g. in [[Azure OCR API]]).\n\n","lastmodified":"2022-04-11T14:32:20.880624273Z","tags":null},"/notes/research/concepts/Image-Similarity-Analysis":{"title":"Untitled Page","content":"# Image Similarity\n#concept \n\nImage Similarity Analysis tries to determine how \"close\" different images are to each other. The similarity of different pictures can be determined using deep learning techniques.","lastmodified":"2022-04-11T14:32:20.880624273Z","tags":null},"/notes/research/concepts/Optical-Character-Recognition":{"title":"Untitled Page","content":"# Optical Character Recognition\n#concept #technique\n\nOptical Character Recognition (OCR) is the conversion of images of text to machine-encoded text. \n\nTraditional OCR processes one character at a time and focuses on typed text. More modern techniques, such as Intelligent Word Recognition, try to use the context a given character is used in and process one word at a time. These more modern techniques also target handwritten text. While OCR nowadays works relatively well for typed text, its accuracy in detecting handwritten text is still limited due to the huge variation in handwritten text (see [[research/concepts/Handwriting Recognition]]).","lastmodified":"2022-04-11T14:32:20.880624273Z","tags":null},"/notes/research/concepts/Semiautomated-Grading":{"title":"Untitled Page","content":"# Semiautomated Grading\n#concept #definition \n\nSemiautomated grading facilitates the grading process by automating and assisting certain aspects of grading. Semiautomated grading involves automated features which may have an impact on the points students are awarded, but also features which are purely assisting the grader, e.g. by making his or her workflow more efficient (see [[research/features/Feature Categories]]). \n\nIn contrast to [[research/concepts/Automated Grading]], humans are still involved in the correction process and verify the suggestions of machines.\n","lastmodified":"2022-04-11T14:32:20.880624273Z","tags":null},"/notes/research/concepts/Text-Similarity":{"title":"Untitled Page","content":"# Text similarity\n#concept \n\nText similarity analysis tries to determine how \"close\" different texts are to each other. There are two different kinds of text similarity:\n- **Lexical** text similarity: Aims to identify how similar two texts on a word level\n- **Semantic** text similarity: Aims to identify how similar two texts are based on the context of each document\n\nTo give an example, the two sentences \"my house is empty\" and \"there is nobody at mine\" have a low lexical similarity but a high semantic similarity. \n\nThese similarities can be identified using Natural Language Processing (NLP) techniques.","lastmodified":"2022-04-11T14:32:20.880624273Z","tags":null},"/notes/research/concepts/Topic-Modeling":{"title":"Untitled Page","content":"# Topic Modeling\n#concept #technique \n\nTopic Modelling is an unsupervised machine learning technique that is capable to detect word and phrase patterns and automatically clustering word groups that best characterize a set of documents.","lastmodified":"2022-04-11T14:32:20.880624273Z","tags":null},"/notes/research/features/Feature-Categories":{"title":"Untitled Page","content":"# Feature Categories\n#definition \n\nWe divide potential features of an semiautomated grading tool into two categories: **assisting** and **automated**\n\nWe define the two categories as follows:\n- Assisting features merely support the grading process and do not have any direct consequences on how many points a given student is awarded for a given question. \n- Automated features may have a direct impact on how many points a student receives for a given question. This includes fully automated point assignment, but also suggestions whether some answer is likely to be right or wrong as the suggestion may bias the examiner.\n\nWe differentiate these two kinds of features for the reason that automated features are prone to controversy and must be handled with care if used in a high-stake environment.","lastmodified":"2022-04-11T14:32:20.880624273Z","tags":null},"/notes/research/features/Feature-Scope":{"title":"Untitled Page","content":"# Feature Scope\n#definition\n\nFeatures of a grading software can be differentiated by the possible scope of their use. While some features can be used are not limited to some type of question/exams, others can only be applied to some type of questions/exams. We classify the scope of the first group as **universal** and the scope of the latter group as **limited**. ","lastmodified":"2022-04-11T14:32:20.880624273Z","tags":null},"/notes/research/features/Overview":{"title":"Untitled Page","content":"# Features\n#feature #overview \n\nThis document summarizes possible feature of a grading tool.\n\n| Feature                                                             | Potential     | Complexity                | Implemented in other tool                                                                 | Adresses                                                              | [Category](research/features/Feature%20Categories.md) | [Scope of use](research/features/Feature%20Scope.md) | Remarks                                                                       |\n| ------------------------------------------------------------------- | ------------- | ------------------------- | ----------------------------------------------------------------------------------------- | --------------------------------------------------------------------- | ----------------------------------------------------- | ---------------------------------------------------- | ----------------------------------------------------------------------------- |\n| [[research/features/definitions/Digitization]]                      | High          | Medium                    | Yes                                                                                       | [[research/challenges/Workload]]                                      | Assisting                                             | Universal                                            | -                                                                             |\n| [[research/features/definitions/Recognition of Student Submission]] | Medium        | Medium                    | Yes                                                                                       | -                                                                     | Assisting                                             | Universal                                            | -                                                                             |\n| [[research/features/definitions/Shuffling]]                         | Low           | Low                       | No                                                                                        | [[research/challenges/Consistency]]                                   | Assisting                                             | Universal                                            | -                                                                             |\n| [[research/features/definitions/Flag Answer]]                       | Low           | Low                       | Yes                                                       | -                                                                     | Assisting                                             | Universal                                            | -                                                                             |\n| [[research/features/definitions/Tags]]                              | Low           | Low                       | Yes                                                      | -                                                                     | Assisting                                             | Universal                                            | -                                                                             |\n| [[research/features/definitions/Anonymity]]                         | Medium        | Low                       | Yes | [[research/challenges/Consistency]]                                   | Assisting                                             | Universal                                            | -                                                                             |\n| [[research/features/definitions/Rubrics]]                           | High          | Medium                    | Yes                               | [[research/challenges/Workload]], [[research/challenges/Consistency]] | Assisting                                             | Universal                                            | -                                                                             |\n| [[research/features/definitions/Manual Feedback]]                   | Medium        | Low                       | Yes  | -                                                                     | Assisting                                             | Universal                                            | -                                                                             |\n| [[research/features/definitions/Grading Workflow]]                  | Medium        | Medium *investigate*      | Yes  | -                                                                     | Assisting                                             | Universal                                            | -                                                                             |\n| [[research/features/definitions/Collaboration]]                     | High          | Medium                    | Yes  | -                                                                     | Assisting                                             | Universal                                            | -                                                                             |\n| [[research/features/definitions/Access to Student Grades]]          | Medium        | Medium                    | No                                                                                        | [[research/challenges/Consistency]]                                   | Assisting                                             | Universal                                            | Information would have to be feeded into the system from university officials |\n| [[research/features/definitions/Statistics]]                        | Medium        | Medium                    | Yes  | -                                                                     | Assisting                                             | Universal                                            | -                                                                             |\n| [[research/features/definitions/Assigning Grades]]                  | Medium        | Medium *investigate*      | No                                                                                        | -                                                                     | -                                                     | Universal                                            | -                                                                             |\n| [[research/features/definitions/Cheating Detection]]                | Medium        | Medium                    | No                                                                                        | -                                                                     | Assisting                                             | Universal                                            | -                                                                             |\n| [[research/features/definitions/Exam Review]]                       | Low           | Medium                    | Yes  | -                                                                     | Assisting                                             | Universal                                            | -                                                                             |\n| [[research/features/definitions/Choice Questions]]                  | High          | Medium                    | Yes  | [[research/challenges/Workload]]                                      | Automated                                             | Limited                                              | -                                                                             |\n| [[research/features/definitions/Manual Grouping]]                   | *investigate* | Medium                    | Yes                                                    | [[research/challenges/Workload]], [[research/challenges/Consistency]] | Assisting                                             | Limited                                              | -                                                                             |\n| [[research/features/definitions/Free Form Annotation]]              | Medium        | Medium                    | Yes                                              | -                                                                     | Assisting                                             | Universal                                            | -                                                                             |\n| [[research/features/definitions/Text Recognition]]                  | Medium        | High                      | No                                                                                        | [[research/challenges/Workload]]                                      | Automated                                             | Limited                                              | State of the art not good enough                                              |\n| [[research/features/definitions/Keyword Extraction]]                | Medium        | Medium *investigate*      | No                                                                                        | [[research/challenges/Workload]]                                      | Automated                                             | Limited                                              | Not clear whether the functionality in itself would be helpful                |\n| [[research/features/definitions/Sorting]]                           | *investigate* | Medium                    | No                                                                                        | [[research/challenges/Workload]]                                      | Automated                                             | Limited                                              | May introduce biases to grading process                                       |\n| [[research/features/definitions/Text Similarity Analysis]]          | Medium        | High *investigate*        | No                                                                                        | [[research/challenges/Workload]]                                      | Automated                                             | Limited                                              | Must work really well in order to be useful                                   |\n| [[research/features/definitions/Code Similarity Analysis]]          | Low           | High                      | Yes                                                    | -                                                                     | Automated                                             | Limited                                              | Out of scope                                                                  |\n| [[research/features/definitions/Automated Grouping]]                | High          | High *investigate*        | No                                                                                        | [[research/challenges/Workload]], [[research/challenges/Consistency]] | Automated                                             | Limited                                              | Must work really well in order to be useful                                   |\n| [[research/features/definitions/Automated Grading Suggestions]]     | Medium        | High *investigate*        | No                                                                                        | [[research/challenges/Workload]]                                      | Automated                                             | Limited                                              | Tedious if it does not work really well                                       |\n| [[research/features/definitions/Fully Automated Grading]]           | High          | Super high (not feasible) | No                                                                                        | [[research/challenges/Workload]]                                      | Automated                                             | Limited                                              | Distrust                                                                      |\n\n","lastmodified":"2022-04-11T14:32:20.880624273Z","tags":null},"/notes/research/features/definitions/Access-to-Student-Grades":{"title":"Untitled Page","content":"# Access to Student Grades\n#feature #assisting \n\nSoftware could provide access to anonymized student grades such that the grader can set an appropriate grading curve. The problem is that examiners do not have any information on the average \"quality\" of students in their course. If the student \"quality\" is not taken into account, students that take courses with many good students are penalized. \n\nSoftware could offer a way to give this information to the graders without violating privacy.\n\n### Prerequisites\nNone\n\n### In use \nNot yet implemented in another tool\n\n","lastmodified":"2022-04-11T14:32:20.880624273Z","tags":null},"/notes/research/features/definitions/Anonymity":{"title":"Untitled Page","content":"# Anonymity\n#feature #assisting\n\nThe identity of examinees is not visible to the person grading the exam. This prevents potentially bad influence of biases (even unconscious ones).\n\nBasic feature for which implementation should be straightforward.\n\n### Prerequisites\nNone\n\n### In use \n- [[research/tools/Gradescope]]\n- [[research/tools/Crowdmark]]\n- [[research/tools/Ans]]\n\n","lastmodified":"2022-04-11T14:32:20.880624273Z","tags":null},"/notes/research/features/definitions/Assigning-Grades":{"title":"Untitled Page","content":"# Assigning Grades\n#feature #assisting \n\nGrading curve can be specified in the tool. Tool already has a predefined set of \"basic\" grading curves.\n\n### Prerequisites\nNone\n\n### In use \nNot yet implemented in another tool\n","lastmodified":"2022-04-11T14:32:20.880624273Z","tags":null},"/notes/research/features/definitions/Automated-Grading-Suggestions":{"title":"Untitled Page","content":"# Automated Grading Suggestions\n#feature #automated\n\nTool automatically provides suggestion for graders, e.g. suggestions whether some criteria is fulfilled. Suggestions still have to be verified by human grader. \n\nCould reduce workload but only if it works well. Would technically be very challenging, expenses probably too high for revenue. Moreover it could be an issue for the consistency of the grading (either because the algorithm is not consistent or because suggestions are not consistently accepted or dismissed).\n\n### Prerequisites\n- [[research/features/definitions/Text Recognition]]\n- [[research/features/definitions/Text Similarity Analysis]]\n\n### In use \nNot yet implemented in another tool\n\n\n","lastmodified":"2022-04-11T14:32:20.880624273Z","tags":null},"/notes/research/features/definitions/Automated-Grouping":{"title":"Untitled Page","content":"# Automated Grouping\n#feature #automated \n\nSimilar answers are automatically grouped such that they can all be graded at once.\n\nPotential would be big if it works well. Already implemented solutions only work for relatively easy tasks which are not on university level (fill in the blank with simple math or one word, see [[research/tools/Gradescope]]).\n\nA possible scope of application would also be single/multiple choice questions.\n\n### Prerequisites\n- [[research/features/definitions/Text Recognition]]\n- [[research/features/definitions/Text Similarity Analysis]]\n- [[research/concepts/Image Similarity Analysis]]\n\n### In use \n- [[research/tools/Gradescope]]\n","lastmodified":"2022-04-11T14:32:20.880624273Z","tags":null},"/notes/research/features/definitions/Cheating-Detection":{"title":"Untitled Page","content":"# Cheating Detection\n#feature #assisting \n\nAutomatically detect suspicious patterns in submissions of students. Probably only useful if exam is online as cheating is less of a problem in in-class exams.\n\n### Prerequisites\nNone\n\n### In use \nNot yet implemented in another tool\n\n","lastmodified":"2022-04-11T14:32:20.880624273Z","tags":null},"/notes/research/features/definitions/Choice-Questions":{"title":"Untitled Page","content":"# Choice Questions\n#feature\n\nWhile open-ended questions are the focus of our project, a software to grade paper-based exams would probably must have to support different type of choice questions (single choice, multiple choice, kprim etc.)\n\n### Prerequisites\n- [[research/features/definitions/Automated Grouping]]\n\n### In use \n- [[research/tools/Gradescope]]\n- [[research/tools/Crowdmark]]\n- [[research/tools/Ans]]\n\n","lastmodified":"2022-04-11T14:32:20.880624273Z","tags":null},"/notes/research/features/definitions/Code-Similarity-Analysis":{"title":"Untitled Page","content":"# Code Similarity Analysis\n#feature #automated \n\nCoding exercises could either be supported with advanced features (e.g., having unit tests run automatically, checking syntax automatically), or we could treat them as normal \"essay\" questions that can be supported with rubrics, etc.\n\nSpecific features tailored to coding exercises are probably out of the scope of our project. Moreover, there already exist reasonable tools in this area.\n\n### Prerequisites\nNone\n\n### In use \n- [[research/tools/Codeexpert]]\n\n","lastmodified":"2022-04-11T14:32:20.880624273Z","tags":null},"/notes/research/features/definitions/Collaboration":{"title":"Untitled Page","content":"# Collaboration\n#feature #assisting \n\nGraders can collaborate on the grading process of (especially) large exams. There are different levels of collaboration: working simultaneously on the same exam but on different exercises (e.g., locking items like in Word), or working simultaneously on the same exercise with synchronization between graders.\n\nAllows sharing of the work between different entities. Probably not requirement for everyone, but a must for some exams.\n\n### Prerequisites\nNone\n\n### In use \n- [[research/tools/Gradescope]]\n- [[research/tools/Crowdmark]]\n- [[Ans]]\n","lastmodified":"2022-04-11T14:32:20.880624273Z","tags":null},"/notes/research/features/definitions/Digitization":{"title":"Untitled Page","content":"# Digitization\n#feature #assisting \n\nIn order to conduct the grading with a tool, the exams must first be digitized. If the exams are paper-based, digitization would probably happen through scanning of the exams.\n\n### Prerequisites\nNone\n\n### In use \n- [[research/tools/Gradescope]]\n- [[research/tools/Crowdmark]]\n- [[research/tools/Ans]]\n\n\n\n","lastmodified":"2022-04-11T14:32:20.880624273Z","tags":null},"/notes/research/features/definitions/Exam-Review":{"title":"Untitled Page","content":"# Exam Review\n#feature #assisting \n\nIn order to provide access to exam to students, exam review functionality could be supported\n\n### Prerequisites\nNone\n\n### In use\n- [[research/tools/Gradescope]]\n- [[research/tools/Crowdmark]]\n- [[research/tools/Ans]]\n","lastmodified":"2022-04-11T14:32:20.880624273Z","tags":null},"/notes/research/features/definitions/Flag-Answer":{"title":"Untitled Page","content":"# Flag Answer\n#feature #assisting \n\nA specific answer can be flagged such that it can be reviewed later\n\n### Prerequisites\nNone\n\n### In use\n- [[research/tools/Ans]]\n\n","lastmodified":"2022-04-11T14:32:20.880624273Z","tags":null},"/notes/research/features/definitions/Free-Form-Annotation":{"title":"Untitled Page","content":"# Free Form Annotation\n#feature #assisting \n\nFree form annotation would allow additional flexibility for the graders. Would be especially useful if graders use tablet.\n\n### Prerequisites\nNone\n\n### In use\n- [[research/tools/Crowdmark]]\n\n","lastmodified":"2022-04-11T14:32:20.880624273Z","tags":null},"/notes/research/features/definitions/Fully-Automated-Grading":{"title":"Untitled Page","content":"# Fully Automated Grading\n#feature #automated \n\nFully automated grading of open-ended questions is to the present day not feasible. On one hand, it is by now technically too hard to provide satisfying results. On the other hand, there is a lot of distrust against this kind of technology.\n\nThere, however, exist tools for the automated grading of other, more structured, exam formats, such as Excel exams (see [[research/tools/DBF Excel Grading]]) and, obviously, multiple choice. Moreover, there exists a branch of research which is concerned with the automatic grading of essays (see [Ke and NG (2019)](research/resources/Ke-and-Ng-2019.pdf))\n","lastmodified":"2022-04-11T14:32:20.880624273Z","tags":null},"/notes/research/features/definitions/Grading-Workflow":{"title":"Untitled Page","content":"# Grading Workflow\n#feature #assisting \n\nSupport the grading workflow by incorporating best practices within the tool, or by providing documentation on the grading process. A good grading workflow may should improve efficiency and increase consistency by helping to overcome cognitive biases such as the [[research/challenges/biases/Halo]].\n\n### Prerequisites\nNone\n\n### In use \n- [[research/tools/Crowdmark]]\n- [[research/tools/Gradescope]]\n- [[research/tools/Ans]]\n\n","lastmodified":"2022-04-11T14:32:20.880624273Z","tags":null},"/notes/research/features/definitions/Grouping":{"title":"Untitled Page","content":"# Grouping \n#feature \n\nSimilar answers can be grouped in order to grade these answers all at once. This could decrease the workload of grading of open questions and improve consistency. Grouping can be done automatically using machine learning techniques or manually. \n\nAn example of an answer group in [Gradescope](research/tools/Gradescope.md) can be found [here](research/tools/images/answer-group-example.PNG). \n\nGrouping has the potential to drastically decrease the workload and improve the consistency of the grading. Both possibilities of [[research/features/definitions/Automated Grouping]] and [[research/features/definitions/Manual Grouping]] must be investigated.\n\nAccording to an [analysis](research/exams/analysis/feature%20usability/Usability%20Grouping.md) of three exams of our department, grouping would potentially be a useful feature for approximately a third of the questions.\n","lastmodified":"2022-04-11T14:32:20.880624273Z","tags":null},"/notes/research/features/definitions/Keyword-Extraction":{"title":"Untitled Page","content":"# Keyword Extraction\n#feature #assisting \n\nKeywords could be extracted from exercises to allow for [[research/features/definitions/Automated Grading Suggestions]] (e.g., granting points on a rubric item when a matching keyword is found), or as a precursor to [[research/features/definitions/Automated Grouping]] (comparing [[research/features/definitions/Text Similarity Analysis]] using similarity in keywords). \n\n### Prerequisites\n- [[research/features/definitions/Text Recognition]]\n\n### In use\nNot yet implemented in another tool\n\n\n","lastmodified":"2022-04-11T14:32:20.880624273Z","tags":null},"/notes/research/features/definitions/Manual-Feedback":{"title":"Untitled Page","content":"# Manual Feedback\n#feature #assisting \n\nGraders can provide a custom note on exercises during grading that is either shown to the student during review or only to other graders. Comments can be reused.\n\nBasic feature which improves transparency, implementation should be straightforward\n\n### Prerequisites\nNone\n\n### In use \n- [[research/tools/Gradescope]]\n- [[research/tools/Crowdmark]]\n\n\n","lastmodified":"2022-04-11T14:32:20.880624273Z","tags":null},"/notes/research/features/definitions/Manual-Grouping":{"title":"Untitled Page","content":"# Manual Grouping\n#feature #assisting \n\nAll of the responses to a given exercise are manually grouped (e.g., using Drag \u0026 Drop) in a step preceding the actual correction process. This should allow grading an exercise across all students in fewer steps.\n\nThere might be different persons responsible for the manual grouping vs. the grading of the exam.\n\n### Prerequisites\nNone\n\n### In use \n- [[research/tools/Gradescope]]\n\n","lastmodified":"2022-04-11T14:32:20.880624273Z","tags":null},"/notes/research/features/definitions/Recognition-of-Student-Submission":{"title":"Untitled Page","content":"# Recognition of Student Submission\n#feature #assisting \n\nAutomatically recognize who handed in the exam.\n\n### Prerequisites\nNone\n\n### In use \n- [[research/tools/Crowdmark]]\n","lastmodified":"2022-04-11T14:32:20.880624273Z","tags":null},"/notes/research/features/definitions/Rubrics":{"title":"Untitled Page","content":"# Rubrics\n#feature #assisting \n\nA rubric is a set of criteria for the grading of an assignment. In order to evaluate a grade, the rubric is applied to each submission. This should ensure consistency and, in case the rubric can also be acessed by the students, transparency. \n\nRubrics can be used when grading manually without technical support, but also when software is in use. \n\nRubrics are highly rated in our [survey](research/survey/Summary.md) and implementation should be relatively straightforward. They adress key challenges in the grading process, first and foremost workload and consistency.\n\n### Prerequisites\nNone\n\n### In use \n- [[research/tools/Gradescope]]\n- [[research/tools/Ans]]\n","lastmodified":"2022-04-11T14:32:20.880624273Z","tags":null},"/notes/research/features/definitions/Shuffling":{"title":"Untitled Page","content":"# Shuffling\n#feature #assisting \n\nThe order of exercises in the grading stack is shuffled to ensure that students are in different grading positions across exercises (e.g., Person 1 is not always graded as the last person in the stack).\n\n### Prerequisites\nNone\n\n### In use \nNot yet implemented in another tool\n","lastmodified":"2022-04-11T14:32:20.880624273Z","tags":null},"/notes/research/features/definitions/Sorting":{"title":"Untitled Page","content":"# Sorting\n#feature #automated \n\nPresent the answers in some sorted order. This should allow the examiner to assess the answer more quickly since he sees a similar answer as before.\n\n### Prerequisites\n- [[research/features/definitions/Text Recognition]]\n- [[research/features/definitions/Keyword Extraction]]\n\n### In use \nNot yet implemented in another tool\n\n","lastmodified":"2022-04-11T14:32:20.880624273Z","tags":null},"/notes/research/features/definitions/Statistics":{"title":"Untitled Page","content":"# Statistics\n#feature #assisting \n\nAnalytics for the graded questions, not neccessarily decreased workload but probably nice to have for examiners. Tool could support basic statistics on the results of each exercise.\n\n### Prerequisites\nNone\n\n### In use \n- [[research/tools/Gradescope]]\n- [[research/tools/Crowdmark]]\n- [[research/tools/Ans]]\n\n","lastmodified":"2022-04-11T14:32:20.880624273Z","tags":null},"/notes/research/features/definitions/Tags":{"title":"Untitled Page","content":"# Tags\n#feature #assisting \n\nTags could be supported in order to filter out answers\n\n### Prerequisites\nNone\n\n### In use \n- [[research/tools/Crowdmark]]\n","lastmodified":"2022-04-11T14:32:20.880624273Z","tags":null},"/notes/research/features/definitions/Text-Recognition":{"title":"Untitled Page","content":"# Text Recognition\n#feature #assisting \n\nMay be required as a prerequisite for other features (e.g. [[research/features/definitions/Grouping]]). Could also be a standalone feature to enable easier reading of the answers. State-of-the art is still far from perfect (see [[research/concepts/Handwriting Recognition]]).\n\n### Prerequisites\nNone\n\n### In use \nNot yet implemented in another tool\n","lastmodified":"2022-04-11T14:32:20.880624273Z","tags":null},"/notes/research/features/definitions/Text-Similarity-Analysis":{"title":"Untitled Page","content":"# Text Similarity Analysis\nCompare texts between different students, e.g. for essays, probably not too useful because we are mainly focused on content, would decrease workload and (possibly) consistency in some cases, technically probably very difficult to implement.\n\n\n### Prerequisites \n- [[research/features/definitions/Text Recognition]]\n\n### In use \nNot yet implemented in another tool\n","lastmodified":"2022-04-11T14:32:20.880624273Z","tags":null},"/notes/research/features/definitions/template":{"title":"Untitled Page","content":"# Insert feature name here\n*Describe the feature, including what it should adress and what possible downsides are*\n\n### Prerequisites\n*If there are any, list other features that are a prerequisiste for the use of this feature.*\n\n### In use \n*If there are any, list tools which incorporate this feature*\n\n### Implementation Ideas (internal only)\n*List ideas how this feature could be implemented, should not be published*\n\n","lastmodified":"2022-04-11T14:32:20.880624273Z","tags":null},"/notes/research/surveys/Initial-Survey":{"title":"Initial Survey","content":"---\n\n# Summary of Survey \"P8: Correction of open-ended exam questions through automatization\"\n\n#survey #summary\n\nThis document summarizes the results of the online-survey on the correction of open-ended exam questions through automatization.\n\n### Who participated?\n\n45 people completed our survey and provided valuable feedback. The below graphs summarize the roles of the participants, how they are involved in the grading process, and in which academic area they conduct their exams.\n\n![png](notes/research/surveys/plots/output_2_0.png)\n\n### How do the participants conduct their exams?\n\nExams at our university differ from each other in several dimensions. Exams can differ regarding their type (e.g. paper-based in lecture hall), what materials the students are allowed to use, and what type of questions the exams contain (e.g. multiple choice). The below graphs show what types of exams, which material use, and which types of questions are most common in the courses of our participants.\n\n![png](notes/research/surveys/plots/output_4_0.png)\n\n### What motivates examiners to include open-ended questions?\n\nThere are several reasons an examiner may include open-ended questions in an exam. One question in our survey focused on what the most beneficial aspects of open-ended questions are.\n\n_In your opinion, what are the most beneficial factors of open-ended questions? Please rank the factors by assigning their relative rank compared to the other options._\n\n![png](notes/research/surveys/plots/output_6_0.png)\n\nAccording to the participants of our survey, the most beneficial aspect of open-ended questions is the possibility to better assess the students' knowledge (average ranking 1.76), followed by the improved quality of the students' answers (e.g. harder to guess, more detailed answers etc) (2.34). The other included aspects are, on average, perceived to be considerably less beneficial. The mean and standard deviation of the ranking of each beneficial aspects can be found in the appendix.\n\n### What challenges arise from open-ended questions?\n\nWhile open-ended questions have beneficial aspects, they also present challenges to examiners. One question focused on how challenging different aspects of grading open-ended questions are.\n\n_In your opinion, how challenging are these factors regarding the correction of open-ended exam questions? Please classify each of the following factors according to their level of challenge._\n\n![png](notes/research/surveys/plots/output_9_0.png)\n\nThe participants perceived the workload related to the grading of open-ended questions as the biggest challenge (average rating 3.95/5), followed by the inconsistency of examiners (e.g. allocation of a different number of points for the \"same\" answer) (3.50) and the formulation of transparent grading criteria (3.20). Other aspects that were perceived as relatively challenging were the systematic assignment of grades (3.13), handwriting that is hard to read (3.07), varying personal constitution (3.05), the quality of the students' responses (3.05), the inconsistency of multiple examiners grading the same exam (2.96), and the misunderstanding of unclear questions (2.93). The other aspects of open-ended question we included in our survey were, on average, perceived to be less challenging. The mean and standard deviation of the rating of each challenge can be found in the appendix.\n\n### How widespread is the use of software for grading open-ended questions among the participants?\n\nOne question in our survey focused on whether our participants already used software for the grading of open-ended questions.\n\n_Have you previously used software with the specific goal of improving the grading process of open-ended questions? If yes, what software have you used? If no, do you think software could help you with the grading process? How?_\n\nOf the 27 participants who have explicitly stated whether they have used software for grading open-ended questions, 10 answered yes (i.e. 37%). Software listed by the participants includes Ans, SEB, EPIS, a tool developed by the Teaching Center to grade Excel exams, and, in one case, even a tool that was developed by a participant himself. Of the people who have never used software, five explicitly stated that they think software would be useful (e.g. \"Software might help in making a scheme of points to be reviewed when choosing the grade for an open question\") and ten explicitly stated that they do not think there exists useful software that facilitates the grading of open-ended questions (e.g. “I would be surprised if there were a software that did a reasonably good job at this”).\n\n### What features do the participant think would be useful?\n\nThe use of software could, at least partially, address some of the challenges arising from open-ended questions. One question in our survey focused on which features of a software would be helpful for the grading process.\n\n_Please classify each of the following concepts according to their importance when focusing on the correction of open-ended exam questions._\n\n![png](notes/research/surveys/plots/output_13_0.png)\n\nThe feature conisdered as the by far the most important by the participants is the possibility to specify consistent correction criteria, a concept known as rubrics (average rating 4.46). Other features considered as relevant are the possibility to automatically group similar answers (3.60), anonymous correction (3.48), text similarity analysis (3.46), cheating detection (3.40), and the possibility to write manual feedback (3.31). The other features we included in our survey were, on average, perceived to be less important. The mean and standard deviation of the rating of each feature can be found in the appendix.\n\n### Additional Remarks of Participants\n\nOur participants shared a lot of very helpful remarks with us. They ranged from suggestions for additional resources (e.g. \"I think it would be interesting to have a general document describing how are failing grades/grading curves used\") to affirmation of the importance of open-ended questions (e.g. \"Having been a student here myself, I think multiple choice questions provide bad incentives for learning; it would be much better if they were replaced with open-ended questions, for which automated tools could be very valuable\"). Moreover, skepticism towards automated solution was widespread in the remarks (e.g. \"I think that open ended exam questions shouldn't be automated\", \"Apart from multiple choice, having computers try to grade exams is a waste of time and resources\").\n\n### Appendix\n\n1. Table with mean and standard deviation of the ranking of different motivations for including open-ended questions in exams:\n\n|                                   | Description in Survey                                                                                                                  | Mean | StdDev |\n|","lastmodified":"2022-04-11T14:32:20.880624273Z","tags":null},"/notes/research/tools/Ans":{"title":"Untitled Page","content":"# Ans\n#tool #overview \n\n### What is Ans?\nAns is a dutch company offering software for online assignments and exams. ANS has already been used at the Faculty of Business, Economics and Informatics for several online exams.\n\n### Features\nThe Ans Software offers different features related to the grading of open-ended exercises which are listed below.\n\n| Feature                                                | Comment                                                                             |\n| ------------------------------------------------------ | ----------------------------------------------------------------------------------- |\n| [Digitization of Exercises and Exams](research/features/definitions/Digitization.md) | Online and paper-based supported, paper-based exams must be created with ANS        |\n| [[research/features/definitions/Rubrics]]                                            | Slightly different terminology than in [[research/tools/Gradescope]] (called points per criterion) |\n| [Collaborative Grading](research/features/definitions/Collaboration.md)              | Answers are processed as a stack                                                    |\n| [[research/features/definitions/Flag Answer]]                                        | Somehow similar to [[research/features/definitions/Tags]]                                                         |\n| [[research/features/definitions/Shuffling]]                                          | Also non-random order possible                                                      |\n| [[research/features/definitions/Manual Feedback]]                                    | -                                                                                   |\n| [[research/features/definitions/Statistics]]                                         | -                                                                                   |\n| [[research/features/definitions/Exam Review]]                                        | -                                                                                   | \n\n\n### Pricing \nSee [webpage](https://ans.app/pricing)\n10€ per student/year","lastmodified":"2022-04-11T14:32:20.884624279Z","tags":null},"/notes/research/tools/Codeexpert":{"title":"Untitled Page","content":"# Code Expert\n#tool #overview\n\n### What is Code Expert?\n\"Code Expert is a ready-to-use and platform independent online IDE that allows thousands of students at ETH Zürich to work on open programming tasks in exercises and exams.\"\n\n### How and where is Code Expert used?\nCode Expert is used for assignments and exams in both introductory and advanced programming classes. At the point of writing, Code Expert is exclusively used at ETH Zurich. The platform is exclusively used for programming classes and it does not aim to be a general exam environment.\n\n### How is Code Expert relevant for our project\nWhile Code Expert is no holistic solution for online exam it may be a source of inspiration. The tool can be tested [here](https://expert.ethz.ch/enroll/SS22/eduHub) (unclear how long this link is valid).","lastmodified":"2022-04-11T14:32:20.884624279Z","tags":null},"/notes/research/tools/Crowdmark":{"title":"Untitled Page","content":"# Crowdmark\n#tool #overview \n\n### What is Crowdmark?\nCrowdmark is a Canadian company founded by a professor of the University of Toronto. Crowdmark offers software to facilitate the examination and grading process.\n\n### Aim of Crowdmark\nSimilar to [[research/tools/Gradescope]], Crowdmark aims to digitize the grading process, improve efficieny and enable more consistent grading and richer feedback.\n\n### Features\nThe Crowdmark Software offers different features related to the grading of open-ended questions which are listed below.\n\n| Feature                                                | Comment                                                                                    |\n| ------------------------------------------------------ | ------------------------------------------------------------------------------------------ |\n| [Digitization of Exercises and Exams](research/features/definitions/Digitization.md) | Online and paper-based (must be created in Crowdmark) possible                             |\n| [Collaborative Grading](research/features/definitions/Collaboration.md)              | Currently graded answers are blocked for others                                                      |\n| [[research/features/definitions/Grading Workflow]]               | Grading split up by exercise                                                               |\n| [[research/features/definitions/Manual Feedback]]                        | Comments be reused and be assigned a point-value such that they can be used similarly to [[research/features/definitions/Rubrics]] |\n| [[research/features/definitions/Free Form Annotation]]                               | -                                                                                          |\n| [[research/features/definitions/Tags]]                                               | -                                                                                          |\n| [[research/features/definitions/Statistics]]                                         | -                                                                                          |\n| [[research/features/definitions/Exam Review]]             | -                                                                                          | \n\n### Quotes\n- \"Independent studies have shown that educators experience up to a 75% productivity gain using Crowdmark\" (see [webpage](https://crowdmark.com/higher-ed/) and [paper](research/tools/documents/Crowdmark-2014.pdf), no \"scientific\" methodology)\n- \"Our experiences with Crowdmark have been highly favourable. We have been able to objectively demonstrate  a reduction in grading time of 30%, which is consistent with graders subjective estimate of marking time reduction averaging 37%.\" (see independent [paper](research/tools/documents/Ostafichuk-Jaeger-2016.pdf)).\n\n### Pricing\nAs of April 11, 2022\n\nEither:\n  - 3.75$ per student/course\n  - 7$ per user/academic year\n  - Possibility for custom pricing when scale is high\n\n","lastmodified":"2022-04-11T14:32:20.884624279Z","tags":null},"/notes/research/tools/DBF-Excel-Grading":{"title":"Untitled Page","content":"# DBF Excel Grading\n#tool #overview\n\n### What is the DBF Excel Grading tool?\nThe Teaching Center of the Department of Banking and Finance (DBF) developed a tool for the automated creation and correction of problem sets and exams solved in Microsoft [[research/exams/design/Question Types/Excel]]. Since 2020 it is sucessfully used in various courses of the Department of Banking and Finance.\n\n### Aim of the DBF Excel Grading Tool\nThe aim of the DBF Excel Grading tool is to facilitate the personalization of problem sets and exams and their fully automated correction. While the personalization makes cheating more difficult, the fully automated correction heavily decreases the workload and mitigates the risk of human errors.\n\n### Features\nThe main features of the tool are the generation of personalized exams and their fully automated correction. All exercises whose solution can be specified using an Excel formula can be graded automatically. Fully open questions (e.g. open text questions) are presented in the output of the tool and can be graded by hand in a seperate step.\n\n| Feature                     | Comment |\n| --------------------------- | ------- |\n| [[research/features/definitions/Digitization]]            | Exams must be created in Microsoft Excel        |\n| [[research/features/definitions/Statistics]]              | Basic statistics for each exercise       |\n| [[research/features/definitions/Fully Automated Grading]] | Fully automated correction based on sample solution       |\n","lastmodified":"2022-04-11T14:32:20.884624279Z","tags":null},"/notes/research/tools/Gradescope":{"title":"Untitled Page","content":"# Gradescope\n#tool #overview \n\n### What is Gradescope?\nGradescope offers EdTech software for exercise/exam management and assisted grading. The software covers the full \"grading cycle\" from the submission of students to the publishing of the grades.\n\n### Aim of Gradescope\nGradescope aims to assist and improve several dimensions of the grading process:\n- Digitization of the whole grading process\n- Standardization of feedback\n- Improvement of efficiency\n- Better insights into grading process through analytics\n\n### Features\nThe Gradescope Software offers different features related to the grading of open-ended exercises which are listed below.\n\n| Feature                                                | Comment                                                                                                                                        |\n| ------------------------------------------------------ | ---------------------------------------------------------------------------------------------------------------------------------------------- |\n| [Digitization of Exercises and Exams](research/features/definitions/Digitization.md) | Online and paper-based supported                                                                                                               |\n| [Collaborative Grading](research/features/definitions/Collaboration.md)              | Stack of ungraded answers which is processed                                                                                                   |\n| [[research/features/definitions/Grading Workflow]]                                   | Grading split up by exercise, grouping before grading                                                                                          |\n| [[research/features/definitions/Manual Feedback ]]                                   | Comments can be reused                                                                                                                         |\n| [[research/features/definitions/Rubrics]]                                            | Can be adjusted on the fly                                                                                                                     |\n| [[research/features/definitions/Free Form Annotation]]                               | -                                                                                                                                               |\n| [[research/features/definitions/Automated Grouping]]                                 | [Scope](research/features/Feature%20Scope.md) seems to be limited: Simple maths or fill in the blank with 1-2 words, constrained to one line, limited to english |\n| [[research/features/definitions/Manual Grouping]]                                    | -                                                                                                                                              |\n| [[research/features/definitions/Statistics]]                                         | -                                                                                                                                              |\n| [[research/features/definitions/Exam Review]]                                        | -                                                                                                                                              |\n\n### Quotes\n- \"Two-thirds of responders report saving 30% or more time relative to their traditional workflow\" (see [paper](research/tools/documents/Gradescope-2017.pdf))\n\n### Pricing\nSee [webpage](https://www.gradescope.com/pricing)\n\nGradescope demands following fees for use of software (as of April 11, 2022):\n- Gradescope Basic \n  - $3 per student/course\n- Gradescope Complete (includes grouping of answers):\n  - $5 per student/course\n- Custom pricing if LMS integration needed\n\n\n\n","lastmodified":"2022-04-11T14:32:20.884624279Z","tags":null},"/notes/research/tools/Overview":{"title":"Untitled Page","content":"# Overview\n#tool #overview \n\nWe created an overview for the grading tools:\n- [[research/tools/Gradescope]]\n- [[research/tools/Codeexpert]]\n- [[research/tools/Crowdmark]]\n- [[research/tools/Ans]]\n- [[research/tools/DBF Excel Grading]]\n- [[research/tools/Graide]] \n\nMoreover we tried to gather evidence for [[efficiency gains]] of [[research/tools/Gradescope]] and [[research/tools/Crowdmark]].\n\nOther tools that would potentially be interesting (but we did not create an overview for) include:\n- [Athene](https://github.com/ls1intum/Athene)\n- [Essay Feedback Tool of H5P](https://h5p.org/content-types/essay)\n- [Pearson Automated Scoring Software](https://www.pearsonassessments.com/large-scale-assessments/k-12-large-scale-assessments/automated-scoring.html)\n- [TestWe](https://testwe.eu/en/)\n","lastmodified":"2022-04-11T14:32:20.884624279Z","tags":null},"/private/private-note":{"title":"Private Stuff","content":"This page doesn't get published!","lastmodified":"2022-04-11T14:32:20.908624312Z","tags":null},"/templates/post":{"title":"{{title}}","content":"","lastmodified":"2022-04-11T14:32:20.908624312Z","tags":null}}