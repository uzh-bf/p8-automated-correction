{"/":{"title":"P-8 Automated Correction","content":"---\n\nWelcome to the knowledge base of the project on _\"Automated Correction of Open-ended Exam Questions\"_ that we currently work on at the [BF Teaching Center](https://www.bf.uzh.ch/de/studies/general/teaching-center.html). Our knowledge base contains a summarization of the work we are doing, as well as the resources we provide for your own application.\n\n## Mission\n\nA key requirement when performing any kind of exam correction is to guarantee a **valid, objective** and **fair** **grading process**. There is often some room for interpretation when grading, especially in **open-ended** exam questions, where students generate answers themselves (i.e., the solution space is much broader than with, e.g., single-choice questions). **Procedural fairness** – referring to the perception of correctability and consistency – is of importance in the design, conduction and correction process of any exam, and is at the core of our project.\n\nOur primary goal is to evaluate whether and how the correction of exams could be supported with **automated tools and procedures**. (Semi-)automated correction of open-ended exam questions has the potential to reduce human subjective judgement, improve fairness for the students, as well as allow lecturers to grade in a more structured and efficient way.\n\n## Milestones\n\nThe primary milestones in our project are the following:\n\n| Timeframe  | Description                                 | Resources                                                  |\n|","lastmodified":"2022-04-10T16:31:12.497025161Z","tags":null},"/notes/research/surveys/Initial-Survey":{"title":"Initial Survey","content":"---\n\n# Summary of Survey \"P8: Correction of open-ended exam questions through automatization\"\n\n#survey #summary\n\nThis document summarizes the results of the online-survey on the correction of open-ended exam questions through automatization.\n\n### Who participated?\n\n45 people completed our survey and provided valuable feedback. The below graphs summarize the roles of the participants, how they are involved in the grading process, and in which academic area they conduct their exams.\n\n![png](notes/research/surveys/plots/output_2_0.png)\n\n### How do the participants conduct their exams?\n\nExams at our university differ from each other in several dimensions. Exams can differ regarding their type (e.g. paper-based in lecture hall), what materials the students are allowed to use, and what type of questions the exams contain (e.g. multiple choice). The below graphs show what types of exams, which material use, and which types of questions are most common in the courses of our participants.\n\n![png](notes/research/surveys/plots/output_4_0.png)\n\n### What motivates examiners to include open-ended questions?\n\nThere are several reasons an examiner may include open-ended questions in an exam. One question in our survey focused on what the most beneficial aspects of open-ended questions are.\n\n_In your opinion, what are the most beneficial factors of open-ended questions? Please rank the factors by assigning their relative rank compared to the other options._\n\n![png](notes/research/surveys/plots/output_6_0.png)\n\nAccording to the participants of our survey, the most beneficial aspect of open-ended questions is the possibility to better assess the students' knowledge (average ranking 1.76), followed by the improved quality of the students' answers (e.g. harder to guess, more detailed answers etc) (2.34). The other included aspects are, on average, perceived to be considerably less beneficial. The mean and standard deviation of the ranking of each beneficial aspects can be found in the appendix.\n\n### What challenges arise from open-ended questions?\n\nWhile open-ended questions have beneficial aspects, they also present challenges to examiners. One question focused on how challenging different aspects of grading open-ended questions are.\n\n_In your opinion, how challenging are these factors regarding the correction of open-ended exam questions? Please classify each of the following factors according to their level of challenge._\n\n![png](notes/research/surveys/plots/output_9_0.png)\n\nThe participants perceived the workload related to the grading of open-ended questions as the biggest challenge (average rating 3.95/5), followed by the inconsistency of examiners (e.g. allocation of a different number of points for the \"same\" answer) (3.50) and the formulation of transparent grading criteria (3.20). Other aspects that were perceived as relatively challenging were the systematic assignment of grades (3.13), handwriting that is hard to read (3.07), varying personal constitution (3.05), the quality of the students' responses (3.05), the inconsistency of multiple examiners grading the same exam (2.96), and the misunderstanding of unclear questions (2.93). The other aspects of open-ended question we included in our survey were, on average, perceived to be less challenging. The mean and standard deviation of the rating of each challenge can be found in the appendix.\n\n### How widespread is the use of software for grading open-ended questions among the participants?\n\nOne question in our survey focused on whether our participants already used software for the grading of open-ended questions.\n\n_Have you previously used software with the specific goal of improving the grading process of open-ended questions? If yes, what software have you used? If no, do you think software could help you with the grading process? How?_\n\nOf the 27 participants who have explicitly stated whether they have used software for grading open-ended questions, 10 answered yes (i.e. 37%). Software listed by the participants includes Ans, SEB, EPIS, a tool developed by the Teaching Center to grade Excel exams, and, in one case, even a tool that was developed by a participant himself. Of the people who have never used software, five explicitly stated that they think software would be useful (e.g. \"Software might help in making a scheme of points to be reviewed when choosing the grade for an open question\") and ten explicitly stated that they do not think there exists useful software that facilitates the grading of open-ended questions (e.g. “I would be surprised if there were a software that did a reasonably good job at this”).\n\n### What features do the participant think would be useful?\n\nThe use of software could, at least partially, address some of the challenges arising from open-ended questions. One question in our survey focused on which features of a software would be helpful for the grading process.\n\n_Please classify each of the following concepts according to their importance when focusing on the correction of open-ended exam questions._\n\n![png](notes/research/surveys/plots/output_13_0.png)\n\nThe feature conisdered as the by far the most important by the participants is the possibility to specify consistent correction criteria, a concept known as rubrics (average rating 4.46). Other features considered as relevant are the possibility to automatically group similar answers (3.60), anonymous correction (3.48), text similarity analysis (3.46), cheating detection (3.40), and the possibility to write manual feedback (3.31). The other features we included in our survey were, on average, perceived to be less important. The mean and standard deviation of the rating of each feature can be found in the appendix.\n\n### Additional Remarks of Participants\n\nOur participants shared a lot of very helpful remarks with us. They ranged from suggestions for additional resources (e.g. \"I think it would be interesting to have a general document describing how are failing grades/grading curves used\") to affirmation of the importance of open-ended questions (e.g. \"Having been a student here myself, I think multiple choice questions provide bad incentives for learning; it would be much better if they were replaced with open-ended questions, for which automated tools could be very valuable\"). Moreover, skepticism towards automated solution was widespread in the remarks (e.g. \"I think that open ended exam questions shouldn't be automated\", \"Apart from multiple choice, having computers try to grade exams is a waste of time and resources\").\n\n### Appendix\n\n1. Table with mean and standard deviation of the ranking of different motivations for including open-ended questions in exams:\n\n|                                   | Description in Survey                                                                                                                  | Mean | StdDev |\n|","lastmodified":"2022-04-10T16:31:12.497025161Z","tags":null},"/private/private-note":{"title":"Private Stuff","content":"This page doesn't get published!","lastmodified":"2022-04-10T16:31:12.5010252Z","tags":null},"/templates/post":{"title":"{{title}}","content":"","lastmodified":"2022-04-10T16:31:12.5010252Z","tags":null}}